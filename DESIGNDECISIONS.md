# Design Decisions

## Workload Domain NET Design Decisions

| ID | Decision | Justification | Implication | Alignment | Notes |
|-|-|-|-|-|-|
| SDDC-WLD-VI-NET-001 | Use vSphere Distributed Switches. | Simplifies management. | Migration from a standard switch to a distributed switch requires a minimum of two physical NICs to maintain redundancy. |  |  |
| SDDC-WLD-VI-NET-002 | Use a single vSphere Distributed Switch per vSphere Cluster. | - Reduces the complexity of the network design.  Reduces the size of the fault domain. | Increases the number of vSphere Distributed Switches that must be managed. | |  |
| SDDC-WLD-VI-NET-003 | Configure the MTU size of the vSphere Distributed Switch to 9,000 for jumbo frames. | - Supports the MTU size required by system traffic types.  - Improves traffic throughput. | When adjusting the MTU packet size, you must also configure the entire network path (VMkernel ports, virtual switches, physical switches, and routers) to support the same MTU packet size. |  |  |
| SDDC-WLD-VI-NET-004 | Use static port binding for all port groups in the shared edge and workload cluster. | Static binding ensures a virtual machine connects to the same port on the vSphere Distributed Switch. This allows for historical data and port level monitoring.  Since the vCenter Server managing the workload domain resides in the management domain, there is no need for an ephemeral port group for vCenter Server recoverability. | None | |  |
| SDDC-WLD-VI-NET-005 | Use the Route based on physical NIC load teaming algorithm for the Management Port Group. | Reduces the complexity of the network design and increases resiliency and performance. | None |  |  |
| SDDC-WLD-VI-NET-006 | Use the Route based on physical NIC load teaming algorithm for the vMotion Port Group. | Reduces the complexity of the network design and increases resiliency and performance. | None |  |  |
| SDDC-WLD-VI-NET-007 | Use the vMotion TCP/IP stack for vSphere vMotion traffic. | By using the vMotion TCP/IP stack, vSphere vMotion traffic can be assigned a default gateway on its own subnet and can go over Layer 3 networks. | In the vSphere Client, the vMotion TCP/IP stack is not available in the wizard for creating a VMkernel network adapter wizard at the distributed port group level. You must create the VMkernel adapter directly on the ESXi host. | |  |
| SDDC-WLD-VI-NET-008 | Enable Network I/O Control on the vSphere Distributed Switch for the workload domain. | Increases resiliency and performance of the network. | If configured incorrectly, Network I/O Control might impact network performance for critical traffic types. | |  |
| SDDC-WLD-VI-NET-009 | Set the share value for management traffic to Normal. | By keeping the default setting of Normal, management traffic is prioritized higher than vSphere vMotion and vSphere Replication but lower than vSAN traffic. Management traffic is important because it ensures that the hosts can still be managed during times of network contention. | None. |  |  |
| SDDC-WLD-VI-NET-010 | Set the share value for vSphere vMotion traffic to Low. | During times of network contention, vSphere vMotion traffic is not as important as virtual machine or storage traffic. | During times of network contention, vMotion takes longer than usual to complete. | |  |
| SDDC-WLD-VI-NET-011 | Set the share value for virtual machines to High. | Virtual machines are the most important asset in the SDDC. Leaving the default setting of High ensures that they always have access to the network resources they need. | None | |  |
| SDDC-WLD-VI-NET-012 | Set the share value for vSphere Fault Tolerance to Low. | This design does not use vSphere Fault Tolerance. Fault tolerance traffic can be set the lowest priority. | None |  |  |
| SDDC-WLD-VI-NET-014 | Set the share value for NFS traffic to Low (25). | Because NFS is used for secondary storage, such as backups and vRealize Log Insight archives, its priority is lower than the priority of the vSAN traffic. | During times of network contention, backups are slower than usual. | |  |
| SDDC-WLD-VI-NET-015 | Set the share value for backup traffic to Low. | During times of network contention, the primary functions of the SDDC must continue to have access to network resources with priority over backup traffic. | During times of network contention, backups are slower than usual. | |  |
| SDDC-WLD-VI-NET-016 | Set the share value for iSCSI traffic to Low | This design does not use iSCSI. iSCSI traffic can be set the lowest priority. | None | |  |
| SDDC-WLD-VI-NET-017 | Set the share value for vSphere Replication traffic to Low (25). | During times of network contention, vSphere Replication traffic is not as important as virtual machine or storage traffic. | During times of network contention, vSphere Replication takes longer and might violate the defined SLA. | |  |

## Workload Domain SDN Design Decisions

| ID | Decision | Justification | Implication | Alignment | Notes |
|-|-|-|-|-|-|
| SDDC-WLD-VI-SDN-001 | Use two ToR switches for each rack. | Supports the use of two 10 GbE (25 GbE or greater recommended) links to each server and provides redundancy and reduces the overall design complexity. | Requires two ToR switches per rack which can increase costs. | |  |
| SDDC-WLD-VI-SDN-002 | Implement the following physical network architecture: - One 25 GbE (10 GbE minimum) port on each ToR switch for ESXi host uplinks.  -  Layer 3 device that supports BGP. | - Guarantees availability during a switch failure.  - Provides support for BGP as the only dynamic routing protocol that is supported by NSX-T Data Center. | - Might limit the hardware choices.  - Requires dynamic routing protocol configuration in the physical network | |  |
| SDDC-WLD-VI-SDN-003 | Do not use EtherChannel (LAG, LACP, or vPC) configuration for ESXi host uplinks | - Simplifies configuration of top of rack switches.  -  Teaming options available with vSphere Distributed Switch and N-VDS provide load balancing and failover.  - EtherChannel implementations might have vendor-specific limitations. | None. | |  |
| SDDC-WLD-VI-SDN-004 | Use a physical network that is configured for BGP routing adjacency | - Supports flexibility in network design for routing multi-site and multi- tenancy workloads.  - Uses BGP as the only dynamic routing protocol that is supported by NSX- T.  - Supports failover between ECMP Edge uplinks. | Requires BGP configuration in the physical network. | |  |
| SDDC-WLD-VI-SDN-005 | Assign persistent IP configurations to each management component in the SDDC with the exception for NSX-T tunnel endpoints (TEPs) that use dynamic IP allocation. | Ensures that interfaces such as management and storage always have the same IP address. In this way, you provide support for continuous management of ESXi hosts by using vCenter Server and for provisioning IP storage by storage administrators. | Requires precise IP address management. | |  |
| SDDC-WLD-VI-SDN-006 | Set the lease duration for the NSX-T Host Overlay network DHCP scope to at least 7 days. | NSX-T Host Overlay VMkernel port IP addresses are assigned by using a DHCP server. -  NSX-T Host Overlay VMkernel ports do not have an administrative endpoint. As a result, they can use DHCP for automatic IP address assignment. IP pools are an option, but the NSX-T administrator must create them. If you must change or expand the subnet, changing the DHCP scope is simpler than creating an IP pool and assigning it to the ESXi hosts.  - DHCP simplifies the configuration of default gateway for Host Overlay VMkernel ports if hosts within same cluster are on separate Layer 2 domains. | Requires configuration and management of a DHCP server. | |  |
| SDDC-WLD-VI-SDN-007 | Use VLANs to separate physical network functions. | - Supports physical network connectivity without requiring many NICs.  - Isolates the different network functions of the SDDC so that you can have differentiated services and prioritized traffic as needed. | Requires uniform configuration and presentation on all the trunks that are made available to the ESXi hosts. | |  |
| SDDC-WLD-VI-SDN-008 | Set the MTU size to at least 1700 bytes (recommended 9000 bytes for jumbo frames) on the physical switch ports, vSphere Distributed Switches, vSphere Distributed Switch port groups, and N-VDS switches that support the following traffic types: - Geneve (overlay)  - vSAN  - vSphere vMotion  - NFS | - Improves traffic throughput.  - Supports Geneve by increasing the MTU size to a minimum of 1600 bytes.  - Geneve is an extensible protocol. The MTU size might increase with future capabilities. While 1600 is sufficient, an MTU size of 1700 bytes provides more room for increasing the Geneve MTU size without the need to change the MTU size of the physical infrastructure. | When adjusting the MTU size, you must also configure the entire network path (VMkernel ports, virtual switches, physical switches, and routers) to support the same MTU size. | |  |
| SDDC-WLD-VI-SDN-009 | Set the MTU size to at least 1700 bytes (recommended 9000 bytes for jumbo frames) on physical inter- availability zone networking components which are part of the networking path between availability zones for the following traffic types. - Geneve (overlay)  - vSAN  - vSphere vMotion  - NFS | - Improves traffic throughput.  - Geneve packets are tagged as do not fragment.  - For optional performance, provides a consistent MTU size across the environment.  - Geneve is an extensible protocol. The MTU size might increase with future capabilities. While 1600 is sufficient, an MTU size of 1700 bytes provides more room for increasing the Geneve MTU size without the need to change the MTU size of the physical infrastructure. | When adjusting the MTU size, you must also configure the entire network path (VMkernel ports, virtual switches, physical switches, and routers) to support the same MTU size. In multi-AZ deployments, the MTU must be configured on the entire network path between AZs. | |  |
| SDDC-WLD-VI-SDN-010 | Configure VRRP, HSRP, or another Layer 3 gateway availability method. | Ensures that the VLANs that are stretched between availability zones are connected to a highly- available gateway if a failure of an availability zone occurs. Otherwise, a failure in the Layer 3 gateway will cause disruption in traffic in the SDN setup. | Requires configuration of a high availability technology for the Layer 3 gateways in the data center. | |  |
| SDDC-WLD-VI-SDN-011 | Deploy three NSX-T Manager nodes for the workload domain in the first cluster in the management domain for configuring and managing the network services for SDDC tenant workloads. | SDDC tenant workloads can be placed on isolated virtual networks, using load balancing, logical switching, dynamic routing, and logical firewalls services. | None. | |  |
| SDDC-WLD-VI- SDN-012 | Deploy each node in the NSX-T Manager cluster for the workload domain as a large- size appliance. | A large-size appliance is sufficient for providing network services to the SDDC tenant workloads. | You must provide enough compute and storage resources in the management domain to support this NSX-T Manager cluster. | |  |
| SDDC-WLD-VI-SDN-013 | Create a virtual IP (VIP) address for the NSX-T Manager cluster for the workload domain. | Provides high availability of the user interface and API of NSX-T Manager. | - The VIP address feature provides high availability only. It does not load balance requests across the cluster.  - When using the VIP address feature, all NSX-T Manager nodes must be deployed on the same Layer 2 network. | |  |
| SDDC-WLD-VI-SDN-014 | Apply VM-VM anti-affinity rules in vSphere Distributed Resource Scheduler (vSphere DRS) to the NSX-T Manager appliances. | Keeps the NSX-T Manager appliances running on different ESXi hosts for high availability. | - You must allocate at least four physical hosts so that the three NSX-T Manager appliances continue running if an ESXi host failure occurs.  -  You must perform additional configuration for the anti-affinity rules. | |  |
| SDDC-WLD-VI-SDN-015 | In vSphere HA, set the restart priority policy for each NSX-T Manager appliance to high. | - NSX-T Manager implements the control plane for virtual network segments. If the NSX-T Manager cluster is restarted, applications that are connected to the NSX-T VLAN or virtual network segments lose connectivity only for a short time until the control plane quorum is re- established.  - Setting the restart priority to high reserves highest for future needs. | If the restart priority for another management appliance is set to highest, the connectivity delays for services will be longer. | |  |
| SDDC-WLD-VI-SDN-016 | When using two availability zones, add NSX-T Manager appliances to the VM group of Availability Zone 1. | Ensures that, by default, the NSX-T Manager appliances are powered on within the primary availability zone hosts group. | None. | |  |
| SDDC-WLD-VI-SDN-017 | Place the appliances of the NSX-T Manager cluster on the management VLAN segment. | - Provides direct secure connection to the ESXi hosts and vCenter Server for edge node management and distributed network services.  - Reduces the number of required VLANs because a single VLAN can be allocated to both, vCenter Server and NSX-T. | None. | |  |
| SDDC-WLD-VI- SDN-018 | Allocate a statically assigned IP address and host name to the nodes of the NSX-T Manager cluster. | Ensures stability across the SDDC, makes it simpler to maintain and track, and to implement a DNS configuration. | Requires precise IP address management. | |  |
| SDDC-WLD-VI- SDN-019 | Configure forward and reverse DNS records for the nodes of the NSX-T Manager cluster for the workload domain, assigning the record to the child domain. | The NSX-T Manager nodes and VIP address are accessible by using fully qualified domain names instead of by using IP addresses only. | You must provide DNS records for the NSX-T Manager nodes for the workload domain. | |  |
| SDDC-WLD-VI- SDN-020 | Configure NTP on each NSX-T Manager appliance. | NSX-T Manager depends on time synchronization. | None. | |  |
| SDDC-WLD-VI-SDN-021 | Use large-sized NSX-T Edge virtual appliances. | The large-sized appliance provides the required performance characteristics for most tenant workloads. The deployment and life cycle management of virtual edges node is simpler. You can migrate virtual machine form factor between physical locations to support advanced deployment topologies such as multiple availability zones. | None. | |  |
| SDDC-WLD-VI-SDN-022 | - Deploy the NSX-T Edge virtual appliances to the shared edge and workload cluster in the workload domain.  - Do not configure a dedicated vSphere cluster for edge nodes. | - Keeps tenant network traffic local to the workload domain.  - Simplifies configuration and minimizes the number of ESXi hosts required for initial deployment. | NSX-T Edge appliances are co-located with tenant workloads. Ensure that tenant workloads do not prevent NSX- T Edge nodes from moving network traffic. | |  |
| SDDC-WLD-VI-SDN-023 | Deploy two NSX-T Edge appliances in an edge cluster in the shared edge and workload cluster. | Creates the NSX-T Edge Cluster for satisfying the requirements for availability and scale. | When additional NSX-T Edge nodes are added, the Resource Pool Memory Reservation must be adjusted. | |  |
| SDDC-WLD-VI-SDN-024 | Create a Resource Pool in the root of the shared edge and workload cluster object for NSX-T Edge appliances. Create a separate Resource Pool in the root of the shared edge and workload vSphere Cluster object for tenant workloads. | - Tenant workloads  must be deployed to a separate Resource Pool at the root of the cluster.  - To ensure adequate resources for tenant and control plane workloads, there must be no virtual machines in the root of the cluster. | Deploying a Resource Pool for NSX-T Edge appliances allows you to ensure the NSX-T Edge Cluster receives adequate compute resources during times of contention. Tenant workloads might not be able to use their allocated memory during times of contention. | |  |
| SDDC-WLD-VI-SDN-025 | Configure the NSX-T Edge Resource Pool with a 64GB memory reservation and high CPU share value. | - Configuring the Resource Pool for NSX-T Edge appliances with a full memory reservation allows you to ensure the NSX-T Edge Cluster receives adequate memory resources during times of contention.  - Setting the CPU share value to high gives priority to NSX-T Edge appliances in times of CPU contention. | Tenant workloads might not be able to use their allocated memory or CPU during times of contention. | |  |
| SDDC-WLD-VI-SDN-026 | Apply VM-VM anti-affinity rules for vSphere DRS to the virtual machines of the NSX-T Edge cluster. | Keeps the NSX-T Edge nodes running on different ESXi hosts for high availability. | You must perform additional tasks to set up the anti-affinity rules. | |  |
| SDDC-WLD-VI-SDN-027 | In vSphere HA, set the restart priority policy for each NSX-T Edge appliance to high. | - NSX-T Edge VMs are part of the north/south data path for overlay segments. High priority order ensure lower priority VMs will have connectivity.  - Setting the restart priority to high reserves highest for future needs. | If the restart priority for another VM is set to highest, the connectivity delays for other appliances will be longer. | |  |
| SDDC-WLD-VI-SDN-028 | Configure all NSX-T Edge nodes as transport nodes. | Enables the participation of NSX-T Edge nodes in the overlay network for delivery of services to the SDDC workloads such as routing and load balancing. | None. | |  |
| SDDC-WLD-VI-SDN-029 | Create an NSX-T Edge Cluster with the default Bidirectional Forwarding Detection (BFD) configuration between the NSX-T Edge nodes in the cluster. | - Satisfies the availability requirements by default.  - Edge nodes must remain available to create services such as NAT, routing to physical networks, and load balancing. | None. | |  |
| SDDC-WLD-VI-SDN-030 | When using two availability zones, add NSX-T Edge appliances to the VM group of Availability Zone 1. | Ensures that, by default, the NSX-T Edge appliances are powered on within the primary availability zone hosts group. | None | |  |
| SDDC-WLD-VI-SDN-031 | Connect the management interface eth0 of each NSX-T Edge node to the management VLAN. | Provides connection to the NSX-T Manager cluster. | None. | |  |
| SDDC-WLD-VI-SDN-032 | - Connect the fp-eth0 interface of each NSX-T Edge appliance to a VLAN trunk port group pinned to physical NIC 0 of the host.  - Connect the fp-eth1 interface of each NSX-T Edge appliance to a VLAN trunk port group pinned to physical NIC 1 of the host.  - Leave the fp-eth2 interface of each NSX-T Edge appliance unused. | Because VLAN trunk port groups pass traffic for all VLANs, VLAN tagging can occur in the NSX-T Edge node itself for easy post- deployment configuration. - By using two separate VLAN trunk port groups, you can direct traffic from the NSX-T Edge node to a particular host network interface and top of rack switch as needed. | None. | |  |
| SDDC-WLD-VI-SDN-033 | Use a single N-VDS in the NSX-T Edge nodes. | - Simplifies deployment of the edge nodes.  - The same N-VDS switch design can be used regardless of edge form factor.  - Supports multiple TEP interfaces in the edge node.  - vSphere Distributed Switch is not supported in the edge node. | None. | |  |
| SDDC-WLD-VI-SDN-034 | Create one uplink profile for the edge node with three teaming policies. - Default teaming policy of load balance source both active uplinks uplink-1 and uplink-2.  - Named teaming policy of failover order with a single active uplink uplink-1 without standby uplinks.  - Named teaming policy of failover order with a single active uplink uplink-2 without standby uplinks. | - An NSX-T Edge node that uses a single N-VDS can have only one uplink profile.  - For increased resiliency and performance, supports the concurrent use of both edge uplinks through both physical NICs on the ESXi hosts.  - The default teaming policy increases overlay performance and availability by using multiple Host Overlay VMkernel ports and appropriate balancing of overlay traffic.  - By using named teaming policies, you can connect an edge uplink to a specific host uplink and from there to a specific top of rack switch in the data center.  - Enables ECMP in each availability zone because the NSX-T Edge nodes can uplink to the physical network over two different VLANs. VLAN ID is required in the uplink profile. Hence, you must create an uplink profile for each VLAN used by the NSX-T Edge virtual machines. | You can use this policy only with ESXi hosts. Edge virtual machines must use the failover order teaming policy. | |  |
| SDDC-WLD-VI-SDN-035 | Use a dedicated VLAN for the NSX-T Edge Overlay network that is segmented from the Host Overlay VLAN. | The NSX-T Edge Overlay network must be isolated from the Host Overlay network to protect the Host Overlay traffic from NSX-T Edge- generated overlay traffic. | - You must have routing between the VLANs for NSX-T Edge Overlay and Host Overlay.  -  You must allocate another VLAN in the data center infrastructure for NSX-T Edge Overlay traffic. | |  |
| SDDC-WLD-VI-SDN-036 | Use SDDC Manager to perform the life cycle management of NSX-T Manager and related components in the workload domain. | Because the deployment scope of SDDC Manager covers the full SDDC stack, SDDC Manager performs patching, update, or upgrade of the workload domain as a single process. | The operations team must understand and be aware of the impact of a patch, update, or upgrade operation by using SDDC Manager. | |  |
| SDDC-WLD-VI-SDN-037 | Deploy an active-active Tier-0 gateway. | Supports ECMP north-south routing on all Edge nodes in the NSX-T Edge cluster. | Active-active Tier-0 gateways cannot provide stateful services such as NAT . | |  |
| SDDC-WLD-VI-SDN-038 | To enable ECMP between the Tier-0 gateway and the Layer 3 devices (ToR switches or upstream devices), create two VLANs. The ToR switches or upstream Layer 3 devices have an SVI on one of the two VLANs and each NSX-T Edge node in the cluster has an interface on each VLAN. | Supports multiple equal-cost routes on the Tier-0 gateway and provides more resiliency and better bandwidth use in the network. | Additional VLANs are required. | |  |
| SDDC-WLD-VI-SDN-039 | Assign a named teaming policy to the VLAN segments to the Layer 3 device pair. | Pins the VLAN traffic on each segment to its target Edge node interface. From there the traffic is directed to the host physical NIC that is connected to the target top of rack switch. | None. | |  |
| SDDC-WLD-VI-SDN-040 | Create a VLAN transport zone for NSX-T Edge uplink traffic. | Enabled the configuration of VLAN segments on the N- VDS in the Edge nodes. | Additional VLAN transport zones are required if the edge nodes are not connected to the same top of rack switch pair. | |  |
| SDDC-WLD-VI-SDN-041 | Use BGP as the dynamic routing protocol. | - Enables the dynamic routing by using NSX-T. NSX-T supports only BGP for dynamic routing.  SDDC architectures with multiple availability zones or multiple regions architectures require BGP. | In environments where BGP cannot be used, you must configure and manage static routes. | |  |
| SDDC-WLD-VI-SDN-042 | Configure the BGP Keep Alive Timer to 4 and Hold Down Timer to 12 between the top of tack switches and the Tier-0 gateway. | Provides a balance between failure detection between the top of rack switches and the Tier-0 gateway and overburdening the top of rack switches with keep-alive traffic. | By using longer timers to detect if a router is not responding, the data about such a router remains in the routing table longer. As a result, the active router continues to send traffic to a router that is down. | |  |
| SDDC-WLD-VI-SDN-043 | Do not enable Graceful Restart between BGP neighbors. | Avoids loss of traffic. On the Tier-0 gateway, BGP peers from all the gateways are always active. On a failover, the Graceful Restart capability increases the time a remote neighbor takes to select an alternate Tier-0 gateway. As a result, BFD- based convergence is delayed. | None. | |  |
| SDDC-WLD-VI-SDN-044 | Enable helper mode for Graceful Restart mode between BGP neighbors. | Avoids loss of traffic. During a router restart, helper mode works with the graceful restart capability of upstream routers to maintain the forwarding table which in turn will forward packets to a down neighbor even after the BGP timers have expired causing loss of traffic. | None. | |  |
| SDDC-WLD-VI-SDN-045 | Enable Inter-SR iBGP routing. | In the event that an NSX-T Edge node as all of its northbound eBGP sessions are down, north-south traffic will continue to flow by routing traffic to a different NSX-T Edge node. | None. | |  |
| SDDC-WLD-VI-SDN-046 | Deploy a Tier-1 gateway and connect it to the Tier-0 gateway. | Creates a two-tier routing architecture. | A Tier-1 gateway can only be connected to a single Tier-0 gateway. In cases where multiple Tier-0 gateways are required, you must create multiple Tier-1 gateways. | |  |
| SDDC-WLD-VI-SDN-047 | Deploy a Tier-1 gateway to the NSX-T Edge cluster. | Enables stateful services, such as load balancers and NAT, for SDDC management components. Because a Tier-1 gateway always works in active- standby mode, the gateway supports stateful services. | None. | |  |
| SDDC-WLD-VI-SDN-048 | Deploy a Tier-1 gateway in non-preemptive failover mode. | Ensures that after a failed NSX-T Edge transport node is back online, it does not take over the gateway services thus causing a short service outage. | None. | |  |
| SDDC-WLD-VI-SDN-049 | When you have two availability zones, extend the uplink VLANs to the top of rack switches so that the VLANs are stretched between both availability zones. | Because the NSX-T Edge nodes will fail over between the availability zones, this ensures uplink connectivity to the top of rack switches in both availability zones regardless of the zone the NSX-T Edge nodes are presently in. | You must configure a stretched Layer 2 network between the availability zones by using physical network infrastructure. | |  |
| SDDC-WLD-VI-SDN-050 | When you have two availability zones, provide this SVI configuration on the top of the rack switches or upstream Layer 3 devices. - In Availability Zone 2, configure the top of rack switches or upstream Layer 3 devices with an SVI on each of the two uplink VLANs.  - Make the top of rack switch SVI in both availability zones part of a common stretched Layer 3 network between the availability zones. | Enables the communication of the NSX-T Edge nodes to both the top of rack switches in both availability zones over the same uplink VLANs. | You must configure a stretched Layer 2 network between the availability zones by using the physical network infrastructure. | |  |
| SDDC-WLD-VI-SDN-051 | When you have two availability zones, provide this VLAN configuration. - Use two VLANs to enable ECMP between the Tier-0 gateway and the Layer 3 devices (top of rack switches or upstream devices).  - The ToR switches or upstream Layer 3 devices have an SVI to one of the two VLANS and each NSX-T Edge node has an interface to each VLAN. | Supports multiple equal-cost routes on the Tier-0 gateway, and provides more resiliency and better bandwidth use in the network. | Extra VLANs are required. | |  |
| SDDC-WLD-VI-SDN-052 | Create an IP prefix list that permits access to route advertisement by any network instead of using the default IP prefix list. | Used in a route map to prepend a path to one or more autonomous system (AS-path prepend) for BGP neighbors in Availability Zone 2. | You must manually create an IP prefix list that is identical to the default one. | |  |
| SDDC-WLD-VI-SDN-053 | Create a route map-out that contains the custom IP prefix list and an AS-path prepend value set to the Tier-0 local AS added twice. | - Used for configuring neighbor relationships with the Layer 3 devices in Availability Zone 2.  -  Ensures that all ingress traffic passes through Availability Zone 1. | You must manually create the route map. The two NSX-T Edge nodes will route north-south traffic through Availability Zone 2 only if the connection to their BGP neighbors in Availability Zone 1 is lost, for example, if a failure of the top of the rack switch pair or in the availability zone occurs. | |  |
| SDDC-WLD-VI-SDN-054 | Instead of using the default IP prefix list, create an IP prefix list that permits access to route advertisement by network 0.0.0.0/0. | Used in a route map to configure local-reference on learned default-route for BGP neighbors in Availability Zone 2. | You must manully create an IP prefix list that is identical to the default one. | |  |
| SDDC-WLD-VI-SDN-055 | Apply a route map-in that contains the IP prefix list for the default route 0.0.0.0/0 and assign a lower local- preference , for example, 80 to the learned default route and a lower local-preference, for example, 90 any routes learned. | - Used for configuring neighbor relationships with the Layer 3 devices in Availability Zone 2.  - Ensures that all egress traffic passes through Availability Zone 1. | You must manually create the route map. The two NSX-T Edge nodes will route north-south traffic through Availability Zone 2 only if the connection to their BGP neighbors in Availability Zone 1 is lost, for example, if a failure of the top of the rack switch pair or in the availability zone occurs. | |  |
| SDDC-WLD-VI-SDN-056 | Configure Availability Zone 2 neighbors to use the newly created route map as In and Out filters respectively. | Makes the path in and out of Availability Zone 2 less preferred because the AS path is longer. As a result, all traffic passes through Availability Zone 1. | The two NSX-T Edge nodes will route north-south traffic through Availability Zone 2 only if the connection to their BGP neighbors in Availability Zone 1 is lost, for example, if a failure of the top of the rack switch pair or in the availability zone occurs. | |  |
| SDDC-WLD-VI-SDN-057 | Enable all ESXi hosts in the workload domain as NSX-T transport nodes. | Enables distributed routing, logical segments, and distributed firewall. | None. | |  |
| SDDC-WLD-VI-SDN-058 | Configure each ESXi host as a transport node without using transport node profiles. | - Enables the participation of ESXi hosts and the virtual machines on them in NSX-T overlay and VLAN networks.  - Transport node profiles can only be applied at the cluster level. Because in an environment with multiple availability zones each availability zone is connected to a different set of VLANs, you cannot use a transport node profile. | You must configure each transport node with an uplink profile individually. | |  |
| SDDC-WLD-VI-SDN-059 | Use a vSphere Distributed Switch for the shared edge and workload cluster that is enabled for NSX-T Data Center. | - Use the existing vSphere Distributed Switch.  - Provides NSX-T logical segment capabilities to support advanced use cases. To use features such as distributed routing, tenant workloads must be connected to NSX-T segments. | Management occurs jointly from the vSphere Client to NSX-T Manager. However, you must perform all network monitoring in the NSX-T Manager user interface or another solution. | |  |
| SDDC-WLD-VI-SDN-060 | To provide virtualized network capabilities to tenant workloads, use overlay networks with NSX-T Edge nodes and distributed routing. | - Creates isolated, multi- tenant broadcast domains across data center fabrics to deploy elastic, logical networks that span physical network boundaries.  - Enables advanced deployment topologies by introducing Layer 2 abstraction from the data center networks. | Requires configuring transport networks with an MTU size of at least 1700 bytes. | |  |
| SDDC-WLD-VI-SDN-061 | Create a single overlay transport zone for all overlay traffic across the workload domain and NSX-T Edge nodes. | - Ensures that overlay segments are connected to an NSX-T Edge node for services and north- south routing.  -  Ensures that all segments are available to all ESXi hosts and NSX-T Edge nodes configured as transport nodes. | None. | |  |
| SDDC-WLD-VI-SDN-062 | Create a single VLAN transport zone for uplink VLAN traffic that is applied only to NSX-T Edge nodes. | Ensures that uplink VLAN segments are configured on the NSX-T Edge transport nodes. | If VLAN segments are needed on hosts, you must create another VLAN transport zone for the host transport nodes only. | |  |
| SDDC-WLD-VI-SDN-063 | Create an uplink profile with the load balance source teaming policy with two active uplinks for ESXi hosts. | For increased resiliency and performance, supports the concurrent use of both physical NICs on the ESXi hosts that are configured as transport nodes. | None. | |  |
| SDDC-WLD-VI-SDN-064 | Use hierarchical two-tier replication on all segments. | Hierarchical two-tier replication is more efficient by reducing the number of ESXi hosts the source ESXi host must replicate traffic to. | | |  |
| SDDC-WLD-VI-SDN-065 | Create one or more region- specific NSX-T virtual network segments for workloads that are assigned to a specific region. | Enables workload mobility within the data center without complex physical network configuration. | Each NSX-T virtual network segment requires a unique IP address space. | |  |
| SDDC-WLD-VI-SDN-066 | Limit the use of local accounts. | - Local accounts are not user specific and do not offer complete auditing from solutions back to users.  - Local accounts do not provide full role-based access control capabilities. | You must use Active Directory for user accounts. | |  |
| SDDC-WLD-VI-SDN-067 | Enable NSX-T Manager integration with your corporate identity source by using the region-specific Workspace ONE Access instance. | - Provides integration with Active Directory for role- based access control. You can introduce authorization policies by assignment of organization and cloud services roles to enterprise users and groups defined in your corporate identity source.  - Simplifies deployment by consolidating the Active Directory integration for the SDDC in single component, that is, Workspace ONE Access. | You must have the region- specific Workspace ONE Access deployed before configuring role-based access in NSX-T Manager. | |  |
| SDDC-WLD-VI-SDN-068 | Use Active Directory groups to grant privileges to roles in NSX-T Data Center. | - Centralizes role-based access control by mapping roles in NSX-T Data Center to Active Directory groups.  - Simplifies user management. | -  You must create the role configuration outside of the SDDC stack.  - You must set the appropriate directory synchronization interval in Workspace ONE Access to ensure that changes meet your recoverability SLAs. | |  |
| SDDC-WLD-VI-SDN-069 | Create an NSX-T Enterprise Admin group rainpole.io\ug- nsx-enterprise-admins in Active Directory and map it to the Enterprise Administrator role in NSX-T Data Center. | Provides administrator access to the NSX-T Manager user interface. | You must maintain the life cycle and availability of the Active Directory group outside of the SDDC stack. | |  |
| SDDC-WLD-VI-SDN-070 | Create an NSX-T Auditor group rainpole.io\ug-nsx- auditors in Active Directory and map it to the Auditor role in NSX-T Data Center. | Provides read-only access account to NSX-T Data Center. | You must maintain the life cycle and availability of the Active Directory group outside of the SDDC stack. | |  |
| SDDC-WLD-VI-SDN-071 | Create more Active Directory groups and map them to roles in NSX-T Data Center according to the business and security requirements of your organization. | Each organization has its own internal business processes. You evaluate the role seperation needs in your business and implement mapping from individual user accounts to Active Directory groups and roles in NSX-T Data Center. | You must maintain the life cycle and availability of the Active Directory group outside of the SDDC stack. | |  |
| SDDC-WLD-VI-SDN-072 | Grant administrators access to both the NSX-T Manager user interface and its RESTful API endpoint. | Administrators interact with NSX-T Data Center by using its user interface and API. | None. |  |  |
| SDDC-WLD-VI-SDN-073 | Configure the passwords for CLI access to NSX-T Manager for the root, admin, and audit users, and account lockout behavior for CLI according to the industry standard for security and compliance of your organization. | Aligns with the industry standard across your organization. | You must run console commands on the NSX-T Manager appliances. | |  |
| SDDC-WLD-VI-SDN-074 | Configure the passwords for access to the NSX-T Edge nodes for the root, admin, and audit users, and account lockout behavior for CLI according to the industry standard for security and compliance of your organization. | Aligns with the industry standard across your organization. | You must run console commands on the NSX-T Edge appliances. | |  |
| SDDC-WLD-VI-SDN-075 | Configure the passwords for access to the NSX-T Manager user interface and RESTful API or the root, admin, and audit users, and account lockout behavior for CLI according to the industry standard for security and compliance of your organization. | Aligns with the industry standard across your organization. | You must run console commands on the NSX-T Manager appliances. |  |  |
| SDDC-WLD-VI-SDN-076 | Replace the default self- signed certificate of the NSX- T Manager instance for the workload domain with a certificate that is signed by a third-party certificate authority. | Ensures that the communication between NSX-T administrators and the NSX-T Manager instance is encrypted by using a trusted certificate. | Replacing the default certificates with trusted CA- signed certificates from a certificate authority might increase the deployment preparation time because you must generate and submit certificates requests. | |  |
| SDDC-WLD-VI-SDN-077 | Use a SHA-2 algorithm or stronger when signing certificates. | The SHA-1 algorithm is considered less secure and has been deprecated. | Not all certificate authorities support SHA-2. |  |  |
| SDDC-WLD-VI-SDN-078 | Use SDDC Manager for NSX- T Manager certificate life cycle management. | Ensures consistent life cycle management across management components in the SDDC. | None | |  |

## Management PHY Design Decisions

| ID | Decision | Justification | Implication | Alignment | Notes |
|-|-|-|-|-|-|
| SDDC-MGMT-PHY-001 | In Region SFO, that is Region A, deploy one or two availability zones to support all SDDC management components and their SLAs. | Supports all SDDC management and compute components for a region. Supports stretched clusters and application-aware failover for high availability between two physical locations. | - Using a single availability zone results in limited redundancy of the overall solution.  - A single availability zone can become a single point of failure and prevent high- availability design solutions in a region. Implementing two availability zones increases the solution footprint and can complicate the operational procedures. | | |
| SDDC-MGMT-PHY-002 | Use two separate power feeds for each rack. | Redundant power feeds increase availability by ensuring that failure of a power feed does not bring down all equipment in a rack. Combined with redundant network connections to a rack and in a rack, redundant power feeds prevent a failure of the equipment in an entire rack. | All equipment used must support two separate power feeds. The equipment must keep running if one power feed fails. If the equipment of an entire rack fails, the cause, such as flooding or an earthquake, also affects neighboring racks. | | |
| SDDC-MGMT-PHY-003 | Mount the compute resources (minimum of 4 ESXi hosts) for the first cluster in the management domain together in one rack. | Mounting the compute resources for the first cluster in the management domain together can ease physical data center design, deployment, and troubleshooting. You only need to provide on- ramp and off-ramp connectivity to physical networks , for example, north- south Layer 3 routing for NSX-T Data Center, to a single rack. NSX-T Edge nodes require external connectivity to physical network devices. Placing edge nodes in the same rack minimizes VLAN spread. | Data centers must have sufficient power and cooling to operate the server equipment according to the selected vendor and products. If the equipment in the entire rack fails, to reduce downtime associated with such an event, you must have a second availability zone . | | |
| SDDC-MGMT-PHY-004 | When using two availability zone, in each availability zone, mount the compute resources (minimum of 4 ESXi hosts) for the first cluster in the management domain together in one rack. | Mounting the compute resources for the first cluster in the management domain together can ease physical data center design, deployment, and troubleshooting. You only need to provide on- ramp and off-ramp connectivity to physical networks , for example, north- south Layer 3 routing for NSX-T Data Center, to a single rack. NSX-T Edge nodes require external connectivity to physical network devices. Placing edge nodes in the same rack minimizes VLAN spread. | Data centers must have sufficient power and cooling to operate the server equipment according to the selected vendor and products. If the equipment in the entire rack fails, to reduce downtime associated with such an event, you must have a second availability zone . | | |

## Management NET Design Decisions

| ID | Decision | Justification | Implication | Alignment | Notes |
|-|-|-|-|-|-|
| SDDC-MGMT-VI-NET-001 | Use vSphere Distributed Switches. | Simplifies management. | Migration from a standard switch to a distributed switch requires a minimum of two physical NICs to maintain redundancy. | | |
| SDDC-MGMT-VI-NET-002 | Use a single vSphere Distributed Switch per cluster. | - Reduces the complexity of the network design.  - Reduces the size of the fault domain. | Increases the number of vSphere Distributed Switches that must be managed. | | |
| SDDC-MGMT-VI-NET-003 | Configure the MTU size of the vSphere Distributed Switch to 9000 for jumbo frames. | - Supports the MTU size required by system traffic types.  -  Improves traffic throughput. | When adjusting the MTU packet size, you must also configure the entire network path (VMkernel ports, virtual switches, physical switches, and routers) to support the same MTU packet size. | | |
| SDDC-MGMT-VI-NET-004 | Use ephemeral port binding for the management port group. | Using ephemeral port binding provides the option for recovery of the vCenter Server instance that is managing the distributed switch. | Port-level permissions and controls are lost across power cycles, and no historical context is saved. | | |
| SDDC-MGMT-VI-NET-005 | Use static port binding for all non-management port groups. | Static binding ensures a virtual machine connects to the same port on the vSphere Distributed Switch. This allows for historical data and port level monitoring . | None | | |
| SDDC-MGMT-VI-NET-006 | Use the Route based on physical NIC load teaming algorithm for the managment port group. | Reduces the complexity of the network design and increases resiliency and performance. | None. | | |
| SDDC-MGMT-VI-NET-007 | Use the Route based on physical NIC load teaming algorithm for the vMotion Port Group. | Reduces the complexity of the network design and increases resiliency and performance. | None. | | |
| SDDC-MGMT-VI-NET-008 | Use the vMotion TCP/IP stack for vSphere vMotion traffic. | By using the vMotion TCP/IP stack, vSphere vMotion traffic can be assigned a default gateway on its own subnet and can go over Layer 3 networks. | In the vSphere Client, the vMotion TCP/IP stack is not available in the wizard for creating a VMkernel network adapter wizard at the distributed port group level. You must create the VMkernel adapter directly on the ESXi host. | | |
| SDDC-MGMT-VI-NET-009 | Enable Network I/O Control on vSphere distributed switch of the management domain cluster. | Increases resiliency and performance of the network. | If configured incorrectly, Network I/O Control might impact network performance for critical traffic types. | | |
| SDDC-MGMT-VI-NET-010 | Set the share value for management traffic to Normal. | By keeping the default setting of Normal, management traffic is prioritized higher than vSphere vMotion and vSphere Replication but lower than vSAN traffic. Management traffic is important because it ensures that the hosts can still be managed during times of network contention. | None. | | |
| SDDC-MGMT-VI-NET-011 | Set the share value for vSphere vMotion traffic to Low. | During times of network contention, vSphere vMotion traffic is not as important as virtual machine or storage traffic. | During times of network contention, vMotion takes longer than usual to complete. | | |
| SDDC-MGMT-VI-NET-012 | Set the share value for virtual machines to High. | Virtual machines are the most important asset in the SDDC. Leaving the default setting of High ensures that they always have access to the network resources they need. | None. | | |
| SDDC-MGMT-VI-NET-013 | Set the share value for vSAN traffic to High. | During times of network contention, vSAN traffic needs a guaranteed bandwidth to support virtual machine performance. | None. | | |
| SDDC-MGMT-VI-NET-014 | Set the share value for vSphere Fault Tolerance to Low. | This design does not use vSphere Fault Tolerance. Fault tolerance traffic can be set the lowest priority. | None. | | |

## Management SDN Design Decisions

| ID | Decision | Justification | Implication | Alignment | Notes |
|-|-|-|-|-|-|
| SDDC-MGMT-VI-SDN-001 | Use two ToR switches for each rack. | Supports the use of two 10 GbE (25 GbE or greater recommended) links to each server and provides redundancy and reduces the overall design complexity. | Requires two ToR switches per rack which can increase costs. | | |
| SDDC-MGMT-VI-SDN-002 | Implement the following physical network architecture: - One 25 GbE (10 GbE minimum) port on each ToR switch for ESXi host uplinks.  - Layer 3 device that supports BGP. | - Provides availability during a switch failure.  - Provides support for BGP as the only dynamic routing protocol that is supported by NSX-T Data Center. | - Might limit the hardware choices.  - Requires dynamic routing protocol configuration in the physical network | | |
| SDDC-MGMT-VI-SDN-003 | Do not use EtherChannel (LAG, LACP, or vPC) configuration for ESXi host uplinks | - Simplifies configuration of top of rack switches.  - Teaming options available with vSphere Distributed Switch and N-VDS provide load balancing and failover.  - EtherChannel implementations might have vendor-specific limitations. | None. | | |
| SDDC-MGMT-VI-SDN-004 | Use a physical network that is configured for BGP routing adjacency | - Supports flexibility in network design for routing multi-site and multi- tenancy workloads.  - BGP is the only dynamic routing protocol that is supported by NSX-T.  - Supports failover between ECMP Edge uplinks. | Requires BGP configuration in the physical network. | | |
| SDDC-MGMT-VI-SDN-005 | Assign persistent IP configurations to each management component in the SDDC with the exception for NSX-T tunnel endpoints (TEPs) that use dynamic IP allocation. | Ensures that endpoints have a persistent management IP address. In VMware Cloud Foundation, you assign storage (vSAN and NFS) and vSphere vMotion IP configurations by using user-defined network pools. | Requires precise IP address management. | | |
| SDDC-MGMT-VI-SDN-006 | Set the lease duration for the TEP DHCP scope to at least 7 days. | NSX-T TEPs are assigned by using a DHCP server. - NSX-T TEPs do not have an administrative endpoint. As a result, they can use DHCP for automatic IP address assignment. If you must change or expand the subnet, changing the DHCP scope is simpler than creating an IP pool and assigning it to the ESXi hosts.  - DHCP simplifies the configuration of default gateway for TEP if hosts within same cluster are on separate Layer 2 domains. | Requires configuration and management of a DHCP server. | | |
| SDDC-MGMT-VI-SDN-007 | Use VLANs to separate physical network functions. | - Supports physical network connectivity without requiring many NICs.  -  Isolates the different network functions of the SDDC so that you can have differentiated services and prioritized traffic as needed. | Requires uniform configuration and presentation on all the trunks that are made available to the ESXi hosts. | | |
| SDDC-MGMT-VI-SDN-008 | Set the MTU size to at least 1, 700 bytes (recommended 9,000 bytes for jumbo frames) on the physical switch ports, vSphere Distributed Switches, vSphere Distributed Switch port groups, and N-VDS switches that support the following traffic types: - Overlay (Geneve)  - vSAN  - vSphere vMotion  - NFS | - Improves traffic throughput.  - Supports Geneve by increasing the MTU size to a minimum of 1,600 bytes.  - Geneve is an extensible protocol. The MTU size might increase with future capabilities. While 1,600 is sufficient, an MTU size of 1,700 bytes provides more room for increasing the Geneve MTU size without the need to change the MTU size of the physical infrastructure. | When adjusting the MTU packet size, you must also configure the entire network path (VMkernel network adapters, virtual switches, physical switches, and routers) to support the same MTU packet size. | | |
| SDDC-MGMT-VI-SDN-009 | Set the MTU size to at least 1,700 bytes (recommended 9000 bytes for jumbo frames) on physical inter- availability zone networking components which are part of the networking path between availability zones for the following traffic types. - Overlay (Geneve)  - vSAN  - vSphere vMotion  - NFS | - Improves traffic throughput.  - Geneve packets are tagged as do not fragment.  - For optional performance, provides a consistent MTU size across the environment.  - Geneve is an extensible protocol. The MTU size might increase with future capabilities. While 1,600 is sufficient, an MTU size of 1,700 bytes provides more room for increasing the Geneve MTU size without the need to change the MTU size of the physical infrastructure. | When adjusting the MTU packet size, you must also configure the entire network path (VMkernel ports, virtual switches, physical switches, and routers) to support the same MTU packet size. In multi-AZ deployments, the MTU must be configured on the entire network path between AZs. | | |
| SDDC-MGMT-VI-SDN-010 | Configure VRRP, HSRP, or another Layer 3 gateway availability method for these networks. - Management  - Edge Overlay | Ensures that the VLANs that are stretched between availability zones are connected to a highly- available gateway if a failure of an availability zone occurs. Otherwise, a failure in the Layer 3 gateway will cause disruption in the traffic in the SDN setup. | Requires configuration of a high availability technology for the Layer 3 gateways in the data center. | | |
| SDDC-MGMT-VI-SDN-011 | Deploy three NSX-T Manager nodes for the management domain in the first cluster in the domain for configuring and managing the network services for SDDC management components. | SDDC management components can be placed on isolated virtual networks, using load balancing, logical switching, dynamic routing, and logical firewalls services. | - You must turn on vSphere HA in the first cluster in the management domain.  - The first cluster in the management domain requires four physical ESXi hosts for vSphere HA and for high availability of the NSX-T Manager cluster. | | |
| SDDC-MGMT-VI-SDN-012 | Deploy each node in the NSX-T Manager cluster for the management domain as a medium- size appliance or larger. | A medium-size appliance is sufficient for providing network services to the SDDC management components. | If you extend the management domain, increasing the size of the NSX-T Manager appliances might be required. | | |
| SDDC-MGMT-VI-SDN-013 | Create a virtual IP (VIP) address for the NSX-T Manager cluster for the management domain. | Provides high availability of the user interface and API of NSX-T Manager. | - The VIP address feature provides high availability only. It it does not load balance requests across the cluster.  - When using the VIP address feature, all NSX-T Manager nodes must be deployed on the same Layer 2 network. | | |
| SDDC-MGMT-VI-SDN-014 | Apply VM-VM anti-affinity rules in vSphere Distributed Resource Scheduler (vSphere DRS) to the NSX-T Manager appliances. | Keeps the NSX-T Manager appliances running on different ESXi hosts for high availability. | - You must allocate at least four physical hosts so that the three NSX-T Manager appliances continue running if an ESXi host failure occurs.  - You must perform additional configuration for the anti-affinity rules. | | |
| SDDC-MGMT-VI-SDN-015 | In vSphere HA, set the restart priority policy for each NSX-T Manager appliance to high. | - NSX-T Manager implements the control plane for virtual network segments too. vSphere HA restarts the NSX-T Manager appliances first so that other virtual machines that are being powered on or migrated by using vSphere vMotion while the control plane is offline lose connectivity only until the control plane quorum is re- established.  - Setting the restart priority to high reserves the highest priority for flexibility for adding services that must be started before NSX-T Manager. | If the restart priority for another management appliance is set to highest, the connectivity delays for management appliances will be longer. | | |
| SDDC-MGMT-VI-SDN-016 | When using two availability zones, add the NSX-T Manager appliances to the virtual machine group for Availability Zone 1. | Ensures that, by default, the NSX-T Manager appliances are powered on a host in the primary availability zone. | None. | | |
| SDDC-MGMT-VI-SDN-017 | Place the appliances of the NSX- T Manager cluster on the management VLAN network, that is, sfo01-m01-cl01-vds01-pg- mgmt. | - Provides direct secure connection to the ESXi hosts and vCenter Server for edge node management and distributed network services.  - Reduces the number of required VLANs because a single VLAN can be allocated to both, vCenter Server and NSX-T. | None. | | |
| SDDC-MGMT-VI- SDN-018 | Allocate a statically assigned IP address and host name to the nodes of the NSX-T Manager cluster. | Ensures stability across the SDDC, makes it simpler to maintain and track, and to implement a DNS configuration. | Requires precise IP address management. | | |
| SDDC-MGMT-VI- SDN-019 | Configure forward and reverse DNS records for the nodes of the NSX-T Manager cluster for the management domain, assigning the record to the child domain in the region. | The NSX-T Manager nodes and VIP address are accessible by using fully qualified domain names instead of by using IP addresses only. | You must provide DNS records for the NSX-T Manager nodes for the management domain in each region. | | |
| SDDC-MGMT-VI- SDN-020 | Configure NTP on each NSX-T Manager appliance. | NSX-T Manager depends on time synchronization. | None. | | |
| SDDC-MGMT-VI-SDN-021 | Use medium-size NSX-T Edge virtual appliances. | - The medium-size appliance provides the performance characteristics for supporting the SDDC management components in the SDDC.  - The deployment and life cycle management of virtual edges node is simpler.  - You can migrate virtual machine form factor between physical locations to support advanced deployment topologies such as multiple availability zones. | None. | | |
| SDDC-MGMT-VI-SDN-022 | - Deploy the NSX-T Edge virtual appliances.  - Do not configure a dedicated vSphere cluster for edge nodes. | - Because of the prescriptive nature of the management domain, resource contention from unknown workloads is minimized  - Simplifies configuration and minimizes the number of ESXi hosts required for initial deployment.  - Keeps the management component located in the same domain and cluster, isolated from tenant workloads. | None. | | |
| SDDC-MGMT-VI-SDN-023 | Deploy two NSX-T Edge appliances in an edge cluster in the first cluster in the management domain. | Creates the NSX-T Edge cluster for satisfying the requirements for availability and scale. | If you add more NSX-T Edge appliances, you must adjust the memory reservation on the edge resource pool. | | |
| SDDC-MGMT-VI-SDN-024 | Apply VM-VM anti-affinity rules for vSphere DRS to the virtual machines of the NSX-T Edge cluster. | Keeps the NSX-T Edge nodes running on different ESXi hosts for high availability. | You must perform additional tasks to set up the anti-affinity rules. | | |
| SDDC-MGMT-VI-SDN-025 | In vSphere HA, set the restart priority policy for each NSX-T Edge appliance to high. | - The NSX-T Edge nodes are part of the north-south data path for overlay segments for SDDC management components. vSphere HA restarts the NSX-T Edge appliances first so that other virtual machines that are being powered on or migrated by using vSphere vMotion while the edge nodes are offline lose connectivity only for a short time.  - Setting the restart priority to high reserves highest for future needs. | If the restart priority for another management appliance is set to highest, the connectivity delays for management appliances will be longer . | | |
| SDDC-MGMT-VI-SDN-026 | Configure all edge nodes as transport nodes. | Enables the participation of edge nodes in the overlay network for delivery of services to the SDDC management components such as routing and load balancing. | None. | | |
| SDDC-MGMT-VI-SDN-027 | Create an NSX-T Edge cluster with the default Bidirectional Forwarding Detection (BFD) configuration between the NSX-T Edge nodes in the cluster. | - Satisfies the availability requirements by default.  - Edge nodes must remain available to create services such as NAT, routing to physical networks, and load balancing. | None. | | |
| SDDC-MGMT-VI-SDN-028 | When using two availability zones, add the NSX-T Edge appliances to the virtual machine group for Availability Zone 1. | Ensures that, by default, the NSX-T Edge appliances are powered on a host in the primary availability zone. | None. | | |
| SDDC-MGMT-VI-SDN-029 | Connect the management interface eth0 of each NSX-T Edge node to the management VLAN. | Provides connection to the NSX-T Manager cluster. | None. | | |
| SDDC-MGMT-VI-SDN-030 | - Connect the fp-eth0 interface of each NSX-T Edge appliance to a VLAN trunk port group pinned to physical NIC 0 of the host.  - Connect the fp-eth1 interface of each NSX-T Edge appliance to a VLAN trunk port group pinned to physical NIC 1 of the host.  - Leave the fp-eth2 interface of each NSX-T Edge appliance unused. | - Because VLAN trunk port groups pass traffic for all VLANs, VLAN tagging can occur in the NSX-T Edge node itself for easy post-deployment configuration.  - By using two separate VLAN trunk port groups, you can direct traffic from the edge node to a particular host network interface and top of rack switch as needed. | None. | | |
| SDDC-MGMT-VI-SDN-031 | Use a single N-VDS in the NSX-T Edge nodes. | - Simplifies deployment of the edge nodes.  - The same N-VDS switch design can be used regardless of edge form factor.  - Supports multiple TEP interfaces in the edge node.  - vSphere Distributed Switch is not supported in the edge node. | None. | | |
| SDDC-MGMT-VI-SDN-032 | Create one uplink profile for the edge node with three teaming policies. - Default teaming policy of load balance source both active uplinks uplink-1 and uplink-2.  - Named teaming policy of failover order with a single active uplink uplink-1 without standby uplinks.  - Named teaming policy of failover order with a single active uplink uplink-2 without standby uplinks. | - An NSX-T Edge node that uses a single N-VDS can have only one uplink profile.  - For increased resiliency and performance, supports the concurrent use of both edge uplinks through both physical NICs on the ESXi hosts.  - The default teaming policy increases overlay performance and availability by using multiple TEPs, and balancing of overlay traffic.  - By using named teaming policies, you can connect an edge uplink to a specific host uplink and from there to a specific top of rack switch in the data center.  - Enables ECMP in each availability zone because the NSX-T Edge nodes can uplink to the physical network over two different VLANs. | You can use this policy only with ESXi hosts. Edge virtual machines must use the failover order teaming policy. | | |
| SDDC-MGMT-VI-SDN-033 | Use a dedicated VLAN for edge overlay that is different from the host overlay VLAN. | Edge overlay network must be isolated from the host overlay network to protect the host overlay from edge- generated overlay traffic. | - You must have routing between the VLANs for edge overlay and host overlay.  - You must allocate another VLAN in the data center infrastructure for edge overlay. | | |
| SDDC-MGMT-VI-SDN-034 | Use SDDC Manager to perform the life cycle management of NSX-T Manager and related components in the management domain. | Because the deployment scope of SDDC Manager covers the full SDDC stack, SDDC Manager performs patching, update, or upgrade of the management domain as a single process. | The operations team must understand and be aware of the impact of a patch, update, or upgrade operation by using SDDC Manager. | | |
| SDDC-MGMT-VI-SDN-035 | Deploy an active-active Tier-0 gateway. | Supports ECMP north-south routing on all Edge nodes in the NSX-T Edge cluster. | Active-active Tier-0 gateways cannot provide stateful services such as NAT. | | |
| SDDC-MGMT-VI-SDN-036 | To enable ECMP between the Tier-0 gateway and the Layer 3 devices (ToR switches or upstream devices), create two VLANs. The ToR switches or upstream Layer 3 devices have an SVI on one of the two VLANS and each Edge node in the cluster has an interface on each VLAN. | Supports multiple equal-cost routes on the Tier-0 gateway and provides more resiliency and better bandwidth use in the network. | Additional VLANs are required. | | |
| SDDC-MGMT-VI-SDN-037 | Assign a named teaming policy to the VLAN segments to the Layer 3 device pair. | Pins the VLAN traffic on each segment to its target Edge node interface. From there the traffic is directed to the host physical NIC that is connected to the target top of rack switch. | None. | | |
| SDDC-MGMT-VI-SDN-038 | Create a VLAN transport zone for Edge uplink traffic. | Enabled the configuration of VLAN segments on the N- VDS in the Edge nodes. | Additional VLAN transport zones are required if the edge nodes are not connected to the same top of rack switch pair. | | |
| SDDC-MGMT-VI-SDN-039 | Use BGP as the dynamic routing protocol. | - Enables the dynamic routing by using NSX-T. NSX-T supports only BGP.  - SDDC architectures with multiple availability zones or multiple regions architectures require BGP. | In environments where BGP cannot be used, you must configure and manage static routes. | | |
| SDDC-MGMT-VI-SDN-040 | Configure the BGP Keep Alive Timer to 4 and Hold Down Timer to 12 between the top of tack switches and the Tier-0 gateway. | Provides a balance between failure detection between the top of rack switches and the Tier-0 gateway, and overburdening the top of rack switches with keep-alive traffic. | By using longer timers to detect if a router is not responding, the data about such a router remains in the routing table longer. As a result, the active router continues to send traffic to a router that is down. | | |
| SDDC-MGMT-VI-SDN-041 | Do not enable Graceful Restart between BGP neighbors. | Avoids loss of traffic. On the Tier-0 gateway, BGP peers from all the gateways are always active. On a failover, the Graceful Restart capability increases the time a remote neighbor takes to select an alternate Tier-0 gateway. As a result, BFD- based convergence is delayed. | None. | | |
| SDDC-MGMT-VI-SDN-042 | Enable helper mode for Graceful Restart mode between BGP neighbors. | Avoids loss of traffic. During a router restart, helper mode works with the graceful restart capability of upstream routers to maintain the forwarding table which in turn will forward packets to a down neighbor even after the BGP timers have expired causing loss of traffic. | None. | | |
| SDDC-MGMT-VI-SDN-043 | Enable Inter-SR iBGP routing. | In the event that an edge node as all of its northbound eBGP sessions are down, north-south traffic will continue to flow by routing traffic to a different Edge node. | None. | | |
| SDDC-MGMT-VI-SDN-044 | Deploy a Tier-1 gateway and connect it to the Tier-0 gateway. | Creates a two-tier routing architecture. | A Tier-1 gateway can only be connected to a single Tier-0 gateway. In cases where multiple Tier-0 gateways are required, you must create multiple Tier-1 gateways. | | |
| SDDC-MGMT-VI-SDN-045 | Deploy a Tier-1 gateway to the NSX-T Edge cluster. | Enables stateful services, such as load balancers and NAT, for SDDC management components. Because a Tier-1 gateway always works in active- standby mode, the gateway supports stateful services. | None. | | |
| SDDC-MGMT-VI-SDN-046 | Deploy a Tier-1 gateway in non-preemptive failover mode. | Ensures that after a failed NSX-T Edge transport node is back online, it does not take over the gateway services thus causing a short service outage. | None. | | |
| SDDC-MGMT-VI-SDN-047 | When you have two availability zones, extend the uplink VLANs to the top of rack switches so that the VLANs are stretched between both availability zones. | Because the NSX-T Edge nodes will fail over between the availability zones, ensures uplink connectivity to the top of rack switches in both availability zones regardless of the zone the NSX-T Edge nodes are presently in. | You must configure a stretched Layer 2 network between the availability zones by using physical network infrastructure. | | |
| SDDC-MGMT-VI-SDN-048 | When you have two availability zones, provide this SVI configuration on the top of the rack switches or upstream Layer 3 devices. -  In Availability Zone 2, configure the top of rack switches or upstream Layer 3 devices with an SVI on each of the two uplink VLANs.  - Make the top of rack switch SVI in both availability zones part of a common stretched Layer 2 network between the availability zones. | Enables the communication of the NSX-T Edge nodes to both the top of rack switches in both availability zones over the same uplink VLANs. | You must configure a stretched Layer 2 network between the availability zones by using the physical network infrastructure. | | |
| SDDC-MGMT-VI-SDN-049 | When you have two availability zones, provide this VLAN configuration. - Use two VLANs to enable ECMP between the Tier-0 gateway and the Layer 3 devices (top of rack switches or upstream devices).  - The ToR switches or upstream Layer 3 devices have an SVI to one of the two VLANS and each NSX-T Edge node has an interface to each VLAN. | Supports multiple equal-cost routes on the Tier-0 gateway, and provides more resiliency and better bandwidth use in the network. | Extra VLANs are required. | | |
| SDDC-MGMT-VI-SDN-050 | Create an IP prefix list that permits access to route advertisement by any network instead of using the default IP prefix list. | Used in a route map to prepend a path to one or more autonomous system (AS-path prepend) for BGP neighbors in Availability Zone 2. | You must manually create an IP prefix list that is identical to the default one. | | |
| SDDC-MGMT-VI-SDN-051 | Create a route map-out that contains the custom IP prefix list and an AS-path prepend value set to the Tier-0 local AS added twice. | - Used for configuring neighbor relationships with the Layer 3 devices in Availability Zone 2.  -  Ensures that all ingress traffic passes through Availability Zone 1. | You must manually create the route map. The two NSX-T Edge nodes will route north-south traffic through Availability Zone 2 only if the connection to their BGP neighbors in Availability Zone 1 is lost, for example, if a failure of the top of the rack switch pair or in the availability zone occurs. | | |
| SDDC-MGMT-VI-SDN-052 | Create an IP prefix list that permits access to route advertisement by network 0.0.0.0/0 instead of using the default IP prefix list. | Used in a route map to configure local-reference on learned default-route for BGP neighbors in Availability Zone 2. | You must manully create an IP prefix list that is identical to the default one. | | |
| SDDC-MGMT-VI-SDN-053 | Apply a route map-in that contains the IP prefix list for the default route 0.0.0.0/0 and assign a lower local- preference , for example, 80, to the learned default route and a lower local-preference, for example, 90 any routes learned. |  - Used for configuring neighbor relationships with the Layer 3 devices in Availability Zone 2.  -  Ensures that all egress traffic passes through Availability Zone 1. | You must manually create the route map. The two NSX-T Edge nodes will route north-south traffic through Availability Zone 2 only if the connection to their BGP neighbors in Availability Zone 1 is lost, for example, if a failure of the top of the rack switch pair or in the availability zone occurs. | | |
| SDDC-MGMT-VI-SDN-054 | Configure Availability Zone 2 neighbors to use the route maps as In and Out filters respectively. | Makes the path in and out of Availability Zone 2 less preferred because the AS path is longer. As a result, all traffic passes through Availability Zone 1. | The two NSX-T Edge nodes will route north-south traffic through Availability Zone 2 only if the connection to their BGP neighbors in Availability Zone 1 is lost, for example, if a failure of the top of the rack switch pair or in the availability zone occurs. | | |
| SDDC-MGMT-VI-SDN-055 | Deploy a standalone Tier-1 gateway to support advanced stateful services such as load balancing for other management components. | Provides independence between north-south Tier-1 gateways to support advanced deployment scenarios. | You must add a separate Tier-1 gateway. | | |
| SDDC-MGMT-VI-SDN-056 | Connect the standalone Tier-1 gateway to the cross-region virtual network. | You connect the Tier-1 gateway manually to the networks it provides services on. | You must connect the gateway to each network that requires load balancing. | | |
| SDDC-MGMT-VI-SDN-057 | Configure the standalone Tier-1 gateway with static routes to the gateways of the networks it is connected to. | Because the Tier-1 gateway is standalone, it does not autoconfigure it routes and you must configure it manually with static routes. | You must configure the gateway for each network that requires load balancing. | | |
| SDDC-MGMT-VI-SDN-058 | Enable all ESXi hosts in the management domain as NSX-T transport nodes. | Enables distributed routing, logical segments, and distributed firewall. | None. | | |
| SDDC-MGMT-VI-SDN-059 | Configure each ESXi host as a transport node without using transport node profiles. | Enables the participation of ESXi hosts and the virtual machines on them in NSX-T overlay and VLAN networks. Transport node profiles can only be applied at the cluster level. Because in an environment with multiple availability zones each availability zone is connected to a different set of VLANs, you cannot use a transport node profile. | You must configure each transport node with an uplink profile individually. | | |
| SDDC-MGMT-VI-SDN-060 | Use a vSphere Distributed Switch for the first cluster in the management domain that is enabled for NSX-T Data Center. | - Use the existing vSphere Distributed Switch.  - Provides NSX-T logical segment capabilities to support advanced use cases. To use features such as distributed routing, tenant workloads must be connected to NSX-T segments. | Management occurs jointly from the vSphere Client to NSX-T Manager. However, you must perform all network monitoring in the NSX-T Manager user interface or another solution. | | |
| SDDC-MGMT-VI-SDN-061 | To provide virtualized network capabilities to management workloads, use overlay networks with NSX-T Edge nodes and distributed routing. | - Creates isolated, multi- tenant broadcast domains across data center fabrics to deploy elastic, logical networks that span physical network boundaries.  - Enables advanced deployment topologies by introducing Layer 2 abstraction from the data center networks. | Requires configuring transport networks with an MTU size of at least 1600 bytes. | | |
| SDDC-MGMT-VI-SDN-062 | Create a single overlay transport zone for all overlay traffic across the management domain and NSX-T Edge nodes. | - Ensures that overlay segments are connected to an NSX-T Edge node for services and north- south routing.  - Ensures that all segments are available to all ESXi hosts and NSX-T Edge nodes configured as transport nodes. | None. | | |
| SDDC-MGMT-VI-SDN-063 | Create a single VLAN transport zone for uplink VLAN traffic that is applied only to NSX-T Edge nodes. | Ensures that uplink VLAN segments are configured on the NSX-T Edge transport nodes. | If VLAN segments are needed on hosts, you must create another VLAN transport zone for the host transport nodes only. | | |
| SDDC-MGMT-VI-SDN-064 | Create an uplink profile with the load balance source teaming policy with two active uplinks for ESXi hosts. | For increased resiliency and performance, supports the concurrent use of both physical NICs on the ESXi hosts that are configured as transport nodes. | None. | | |
| SDDC-MGMT-VI-SDN-065 | Use hierarchical two-tier replication on all segments. | Hierarchical two-tier replication is more efficient by reducing the number of ESXi hosts the source ESXi host must replicate traffic to. | | | |
| SDDC-MGMT-VI-SDN-066 | Create one or more cross- region NSX-T virtual network segments for management application components which require mobility between regions. | Enables management workload mobility without complex physical network configuration. | Each NSX-T virtual network segment requires a unique IP address space. | | |
| SDDC-MGMT-VI-SDN-067 | Create one or more region- specific NSX-T virtual network segments for management application components that are assigned to a specific region. | Enables workload mobility within the data center without complex physical network configuration. | Each NSX-T virtual network segment requires a unique IP address space. | | |
| SDDC-MGMT-VI-SDN-068 | Limit the use of local accounts. | - Local accounts are not user specific and do not offer complete auditing from solutions back to users.  -  Local accounts do not provide full role-based access control capabilities. | You must use Active Directory for user accounts. | | |
| SDDC-MGMT-VI-SDN-069 | Enable NSX-T Manager integration with your corporate identity source by using the region-specific Workspace ONE Access instance. | - Provides integration with Active Directory for role- based access control. You can introduce authorization policies by assignment of organization and cloud services roles to enterprise users and groups defined in your corporate identity source.  Simplifies deployment by consolidating the Active Directory integration for the SDDC in single component, that is, Workspace ONE Access. | You must have the region- specific Workspace ONE Access deployed before configuring role-based access in NSX-T Manager. | | |
| SDDC-MGMT-VI-SDN-070 | Use Active Directory groups to grant privileges to roles in NSX-T Data Center. | - Centralizes role-based access control by mapping roles in NSX-T Data Center to Active Directory groups.  -  Simplifies user management. | - You must create the role configuration outside of the SDDC stack.  - You must set the appropriate directory synchronization interval in Workspace ONE Access to ensure that changes are available within a reasonable period. | | |
| SDDC-MGMT-VI-SDN-071 | Create an NSX-T Enterprise Admin group rainpole.io\ug- nsx-enterprise-admins in Active Directory and map it to the Enterprise Administrator role in NSX-T Data Center. | Provides administrator access to the NSX-T Manager user interface. | You must maintain the life cycle and availability of the Active Directory group outside of the SDDC stack. | | |
| SDDC-MGMT-VI-SDN-072 | Create an NSX-T Auditor group rainpole.io\ug-nsx- auditors in Active Directory and map it to the Auditor role in NSX-T Data Center. | Provides read-only access account to NSX-T Data Center. | You must maintain the life cycle and availability of the Active Directory group outside of the SDDC stack. | | |
| SDDC-MGMT-VI-SDN-073 | Create more Active Directory groups and map them to roles in NSX-T Data Center according to the business and security requirements of your organization. | Each organization has its own internal business processes. You evaluate the role seperation needs in your business and implement mapping from individual user accounts to Active Direcotry groups and roles in NSX-T Data Center. | You must maintain the life cycle and availability of the Active Directory group outside of the SDDC stack. | | |
| SDDC-MGMT-VI-SDN-074 | Restrict end-user access to both NSX-T Manager user interface and its RESTful API endpoint. | The workloads in the management domain are not end-user workloads. | End users have access only to endpoint components. | | |
| SDDC-MGMT-VI-SDN-075 | Configure the passwords for CLI access to NSX-T Manager for the root, admin, and audit users, and account lockout behavior for CLI according to the industry standard for security and compliance of your organization. | Aligns with the industry standard across your organization. | You must run console commands on the NSX-T Manager appliances. Some commands like "set auth-policy cli lockout-period  \<lockout-period\>" and "set auth-policy cli max-auth-failures \<auth-failures\>" are usually part of implications | | |
| SDDC-MGMT-VI-SDN-076 | Configure the passwords for access to the NSX-T Edge nodes for the root, admin, and audit users, and account lockout behavior for CLI according to the industry standard for security and compliance of your organization. | Aligns with the industry standard across your organization. | You must run console commands on the NSX-T Edge appliances. | | |
| SDDC-MGMT-VI-SDN-077 | Configure the passwords for access to the NSX-T Manager user interface and RESTful API or the root, admin, and audit users, and account lockout behavior for CLI according to the industry standard for security and compliance of your organization. | Aligns with the industry standard across your organization. | You must run console commands on the NSX-T Manager appliances. | | |
| SDDC-MGMT-VI-SDN-078 | Replace the default self- signed certificate of the NSX- T Manager instance for the management domain with a certificate that is signed by a third-party certificate authority. | Ensures that the communication between NSX-T administrators and the NSX-T Manager instance is encrypted by using a trusted certificate. | Replacing the default certificates with trusted CA- signed certificates from a certificate authority might increase the deployment preparation time because you must generate and submit certificates requests. | | |
| SDDC-MGMT-VI-SDN-079 | Use a SHA-2 algorithm or stronger when signing certificates. | The SHA-1 algorithm is considered less secure and has been deprecated. | Not all certificate authorities support SHA-2. | | |
